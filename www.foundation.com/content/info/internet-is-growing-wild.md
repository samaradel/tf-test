+++
# Don't Remove Title!
title =  "info"
date = "2018-07-02T12:54:46+02:00"
img = "../img/internet-is-growing-480x320.jpg"
categories = ["uncategorized"]
header = "Internet is growing wild"
desc = "Internet is growing in a way which is not sustainable, there needs to be an alternative"
type = "blog"
+++

Internet is growing in a way which is not sustainable, there needs to be an alternative.

### internet capacity needs to become local

Big majority of users in the world connect to servers not being present in their region and as such experience higher cost &amp; low performance.
                            
According to a study of&nbsp;[datacentermap.com](http://www.datacentermap.com/) &nbsp;80% of the datacenters of internet service providers are based in the US and Europe. The rest of the world has scarce resources dotted around territories.

As a consequence most internet users use internet based services running on infrastructure which is located far away from their physical point of presence and most likely outside their country borders. This decreases the general end user experience (latency) but also adds unnecessary costs for actually transporting information back and forth and for enterprises it adds legislation and compliance headaches.

### need for education
                      
On a humanitarian level the opportunities for creativity, learning and development of new generations is influenced in a negative way by not having access to good, affordable internet services keeping the status quo between the developed and less developed world. The United Nations have declared internet access a global&nbsp;[human right](http://www.businessinsider.com/un-says-internet-access-is-a-human-right-2016-7?international=true&amp;r=US&amp;IR=T). See UN resolution&nbsp;[here](https://www.article19.org/data/files/Internet_Statement_Adopted.pdf).

### more performance required
                            
Compute &amp; Storage vendors create poor performing solutions. There has been now year on year improvements of hardware following&nbsp;[Mooreâ€™s law](https://en.wikipedia.org/wiki/Moore%27s_law). While this has allowed ThreeFold to progress and innovate it has also lead to software developers taking these advances for granted and cutting corners where they could creating sub optimals code and allowing software components to be layered on top of each other to achieve certain functionality of behavior.

The inefficiencies have now lead to a situation where organically grown IT architecture are immensely complex and use a variety of components from different soft and hardware vendors integrates by a so called integrator. The overall effort and cost involved to create, operate and maintain such architectures is growing continuously and requiring an ever increasing budget and resourcing to continue.

If it could go down to the core algorithms and take another looks at these, innovate at the heart of technology instead of applying patches and pain killers we would be able to create a lot more end user capacity that what systems provide today. The result of this is that systems will last longer and do not have to be replaced by faster ones, it will take a lot less engineers to create, operate and maintain these systems and overall they will present a more stable and reliable platform achieving higher levels of uptime.

Only upsides, right? Well a huge downside of such an approach is that vendors will make less revenue and more importantly less margin as system will run for longer, more stable and requires less updates. Why would vendors innote at the core of their solutionâ€¦â€¦.

### more power efficiency required

Ten times more power efficiency can be achieved in e.g. storage systems.

Today global Internet infrastructure requires enormous amounts of energy â€“ well north of the entire annual electricity consumption of the United Kingdom â€“ ranking among the more pollutive industries globally (similar to airlines!)

ThreeFold believes IT can do a lot better â€“ in fact ThreeFold believes it can reduce the Internetâ€™s carbon footprint by 10 times compared to other industry standard IT capacity producing solutions.

Power consumption is a function of better compute and storage performance requiring more racks and more cooling. The solutions achieve roughly 3 times the performance per rack (so it uses fewer racks) â€“ and the racks require less energy than typical racks in the industry ðŸ™‚

Read more at this blog:&nbsp;[10x times power savings, is this possible?](https://threefoldtoken.com/information/10-times-power-savings/).

### internet capacity is expensive

Biggest cost of running IT architectures are people

Todayâ€™s complex, built out of â€˜band aid and patchesâ€™ point solutions, organically grown and badly documented IT infrastructures need an armada of people to keep it ticking. Find an example IT budget&nbsp;[here](http://www.gartner.com/downloads/public/explore/metricsAndTools/ITBudget_Sample_2012.pdf). Even though this is an example budget one can learn a lot from the trend that are presented in the example:

* On average an IT budget takes 5% of overall revenues.
* IT consumes 6.5% of the total number of FTE in the company of which 85% are insource and 15% is on payroll. This means that the enterprise doesnâ€™t retain internal know how how to operate their IT.
* Just under 50% of the IT budget is spent on Infrastructure and operations and a similar amount is spent on Applications. A mere 5% is considered to be internal overhead within the IT department.
* Around 65% of the IT budget is spend on resources and services, around 35% is spend on Hard and Software.

These figures presents industry average number but they still paint a troubling picture that the cost of IT are a sizeable part of an overall budget and most spend is going to have the right knowledge skills insourced to the organisation to run the core IT architecture on which the whole company operations rely â€“ for any other discipline in any organisation this would present an unacceptable risk to the business and itâ€™s continuity â€“ strangely not for IT.

### people are source of downtime, we need more intelligence

Biggest source of downtime in computer systems are people.

Getting people involved in fixing infrastructure problems creates the risk of accidentally causing more system downtime. A very recent example on this hit a large&nbsp;[organisation](https://aws.amazon.com/message/41926/)&nbsp;providing cloud services.

20+ years ago when internet datacenters came into existence next to telecom points of presence (POPâ€™â€™) the level of complication in architecting, building and maintaining these infrastructures exploded. From an already reasonable complicated technology setup to transport packets of data around the globe these information warehouses were created where data was uploaded to, processed and the obtained results send back to end users often many hundreds of kilometers     away.

Managing a datacenter that contains solutions for transporting information, for storing information and to process information is not an easy feat and the growth of data volume uploaded, stored and processed has exponentially increased. The number of technologies invented and implemented processing and storing this information has also exploded which as a combination results in a double exponential growth in complexity to architect, build operate and maintain these.

The time has come that we refly on people not anymore to do the right thing in case of emergency â€“ the complexity is overwhelming and the dependency on this information being available is huge. Manual deployments and operational responsibility by people is not providing the agility and speed to keep up with the continuous exponential growth of the industry. It is time to take the human element out of Information Technology and let smart systems take over.

### internet is growing too fast, we need better solutions

The current growth of internet needs 4000+ new big data centers of computer systems.

The global data growth will reach more than 40 Zetabytes by 2020 which represents an average year over year growth from 42% starting at 4.5 Zetabytes in 2013*. To host all this data over 4000 new big data centers need to be built. To achieve this goal, $ trillion USD investment capital are needed and land totalling the size of the UK would be needed&nbsp;[e.g. siemens innovation strategy](https://www.siemens.com/innovation/en/home/innovation-strategy/driving-forward-digitalization.html)